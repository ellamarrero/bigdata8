---
title: "Homework 1"
author: "Ella Marrero, David Morales, Brandon Powell, Joon Chae"
date: "4/6/2021"
output: 
  html_document:
    toc: true 
    toc_float: true
    theme: united
    highlight: haddock
---

```{r setup, include=FALSE}
library(tidyverse)
library(parallel)
library(kableExtra)
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE) 
```

# Setup
First, we will load the data: 
```{r}
data = read.table("https://codowd.com/bigdata/hw1/Review_subset.csv",header=TRUE) # reviews
words = read.table("https://codowd.com/bigdata/hw1/words.csv") # words
words = words[,1]
doc_word = read.table("https://codowd.com/bigdata/hw1/word_freq.csv") 
# word frequency (text-word pairing)
colnames(doc_word) = c("Review ID","Word ID","Times Word")
```

Then, we will create a matrix (marginal regression screening)
```{r, cache=T}
# Create a matrix of word presence
spm = matrix(0L,nrow = nrow(data),ncol=length(words))
for (index in 1:nrow(doc_word)) {
  i = doc_word[index,1]
  j = doc_word[index,2]
  spm[i,j] = doc_word[index,3]
}
colnames(spm) = words
```

Then, we will simplify our matrix from counts by creating a dense dataframe of word presence, as well as add a Y variable (stars, rating by reviewer) for use later. 

```{r}
# define vars
P = as.data.frame(as.matrix(spm>0)) 
stars = data$Score

# runs marginal regressions 
margreg = function(p){
	fit = lm(stars ~ P[[p]])
	sf = summary(fit)
	return(sf$coef[2,4]) # return p-value of coef
}
```

With this density matrix, we can calculate p-values for every word (how significantly the word predicts star rating, with the above regression equation).  
```{r bigboi, eval=T}
ncores = detectCores()-1 # remove 1 core
# Make a cluster
cl = makeCluster(ncores) 
# Export data to the cluster
clusterExport(cl,c("stars","P")) 
# Run the regressions in parallel
pvals = parSapply(cl,1:length(words),margreg)
# Turn off cluster
stopCluster(cl)
# assign word names to each p-value column
names(pvals) = words 
```

# Homework Questions:

## Q1
First, we will plot the p-values.

```{r, pvalue hist}
ggplot()+
  geom_histogram(aes(pvals, fill = ..count..), binwidth = 0.05)+
  labs(title = 'Histogram of p-values', x = 'p-value', y = 'Frequency')+
  theme_classic()
```

The distribution is extremely right skewed, with a slight bimodal distribution (peaks at 0.01 and 0.1/0.2). The skewness suggests that an abundance of the word frequencies have a relatively strong correlation to review rating.

## Q2
From the p-value matrix we calculated, we can count the number of tests that are significant at the $\alpha=0.05$ and $0.01$:  

```{r}
less_01 <- sum(pvals <= 0.01)
less_05 <- sum(pvals <= 0.05)
```

```{r, echo= FALSE}
signif <- c("p < 0.01", "p < 0.05")
pval_count <- c(less_01, less_05)
tibble(signif, pval_count) %>%
  rename("Significance Level" = signif,
         "Number of Tests" = pval_count)%>%
  kable() %>%
  kable_styling(bootstrap_options = 'hover')
```

## Q3
Now, we will calculate the p-value cutoff for the 1% (q = 1) False Discovery Rate (FDR), and plot the rejection region.

<center>
Benjamini & Hochberg: $\quad \alpha^{*}= \max\{p_{k}:p_{k}\leq q \frac{k}{K}\}$  
\[q = 0.01 \quad K = 1125\]

\[\max \{p_{(k)} : \frac{1125 \times p_{(k)}}{k}\leq 0.01\}\]
</center>

```{r}
# function for calculating FDR cutoff, given q
fdr_cut_c <- function(pvals, q){
  pvals = pvals[!is.na(pvals)]
  K = length(pvals)
  k = rank(pvals, ties.method="min")
  alpha = max(pvals[ pvals<= (q*k/K) ])
  alpha
}
# calculating FDR cutoff for FDR = 1%, storing
cutoff01 <- fdr_cut_c(pvals, 0.01)
```

```{r}
K = length(pvals)
q = 0.01
pk = sort(pvals)
k = 1:length(pk)
plot_w = 1:400


ggplot() +
  geom_point(aes(k[plot_w], pk[plot_w]), size = 0.1)+
  geom_line(aes(k[plot_w], ((q/K)*k)[plot_w], color = 'FDR = 0.01'))+
  geom_area(aes(k[plot_w], ((q/K)*k)[plot_w], 
                fill = 'rejection area'), alpha = 0.25)+
  
  labs(title = "FDR = 1%", 
       subtitle = paste("Smallest", length(plot_w), "p-values"),
       x = 'Discoveries under p-value (k)', y = 'p-value')+
  theme_classic() +
  theme(legend.title = element_blank(), legend.position = (c(0.2,0.7))) +
  scale_color_manual(values = c('blue'), labels = c('FDR = 0.01'))+
  scale_fill_manual(values = c('green'), labels = c('rejection area'))
```

The p-value cutoff for 1% FDR is *`r cutoff01`.*  

## Q4
```{r}
num_disc <- sum(pvals<=cutoff01)
p_k <- K*cutoff01
```

We find `r num_disc` discoveries at q = 0.01. We expect `r p_k` to be false (= $P_{(k)}K$).  

## Q5

```{r}
# extract top 10 words from p-value df
o = order(pvals)
top10 <- pvals[o][1:10]
top10 <- cbind(rownames(top10), top10)
top10 <- cbind(word = rownames(top10), top10)
rownames(top10) <- 1:nrow(top10)
```

The 10 most significant words are:
```{r, echo = FALSE}
kable(top10, col.names = c("Word", "Significance Level"))%>%
  kable_styling(bootstrap_options = 'hover')
```

```{r}
# boolean variable to print if additive is significant or not
ad_sig <- pvals['addictive'] < cutoff01
sig_text = c('is not', 'is')
cat(paste("Addictive", sig_text[ad_sig + 1], 'significant.'))
```

These results do make sense to us, as most of these words seem to have a clear connection (intuitively) to review rating. However, more neutral words like 'but', 'same', and 'not' are not as obvious in their connection to review rating. 


## Q6
What are the advantages and disadvantages of our FDR analysis? (4 sentence max)  

<b>Advantages</b>
<ul>
<li>We can be more confident that the words we're looking at don't have a false positive (i.e. insignificant but appears significant) effect on review rating</li>
<li>It doesn't collapse as n increases, unlike the Bonferonni correction</li>
  <ul>
  <li>It controls for a low proportion of type 1 errors instead of guarding against them (also could be disadvantageous)</li>
  </ul>
</ul>
***
<b>Disadvantages</b>
<ul>
<li>Fails to account for n-gram in text processing -- having 2 or more words grouped together changes their meaning</li>
<li> Certain reviewers with extreme word choices may have written a larger proportion of reviews. This would result in us predicting more of these  reviewers' ratings, not ratings generally</li>
  <ul>
  <li><em>Could be accounted for by using reviewer ID in our model</em></li>
   </ul>
<li>Self-selection could be producing a higher proportion of negative reviewers</li>
</ul>

